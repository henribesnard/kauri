# Architecture de Classification d'Intention KAURI

## Vue d'ensemble

Le syst√®me KAURI utilise maintenant une architecture avanc√©e bas√©e sur **LangGraph** et **LLM-based intent classification** au lieu de patterns statiques.

## Pourquoi cette approche ?

### Probl√®me des patterns statiques
‚ùå Ne couvre pas tous les cas possibles
‚ùå N√©cessite maintenance manuelle
‚ùå Pas d'apprentissage ou d'adaptation
‚ùå Faux positifs/n√©gatifs fr√©quents

### Solution : Intent Classifier Agent
‚úÖ Classification dynamique par LLM
‚úÖ S'adapte √† toutes les formulations
‚úÖ Haute pr√©cision avec raisonnement
‚úÖ Extensible sans code changes

---

## Architecture Workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Query                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CLASSIFY_INTENT NODE                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Intent Classifier Agent (LLM: gpt-4o-mini)          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Prompt: Analyser l'intention de l'utilisateur      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Output: {                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    intent_type: "general_conversation" | "rag_query" ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  | "clarification",                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    confidence: 0.0-1.0,                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    reasoning: "Explication..."                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  }                                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ROUTE_BY_INTENT (Conditional Edge)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                ‚îÇ                    ‚îÇ
      ‚ñº                ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DIRECT        ‚îÇ ‚îÇ RETRIEVE &       ‚îÇ ‚îÇ ASK             ‚îÇ
‚îÇ RESPONSE      ‚îÇ ‚îÇ GENERATE         ‚îÇ ‚îÇ CLARIFICATION   ‚îÇ
‚îÇ               ‚îÇ ‚îÇ                  ‚îÇ ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ No RAG      ‚îÇ ‚îÇ ‚Ä¢ Hybrid Search  ‚îÇ ‚îÇ ‚Ä¢ Message asking‚îÇ
‚îÇ ‚Ä¢ LLM only    ‚îÇ ‚îÇ ‚Ä¢ Reranking      ‚îÇ ‚îÇ   for more      ‚îÇ
‚îÇ ‚Ä¢ Fast        ‚îÇ ‚îÇ ‚Ä¢ Context        ‚îÇ ‚îÇ   context       ‚îÇ
‚îÇ               ‚îÇ ‚îÇ ‚Ä¢ LLM with docs  ‚îÇ ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                  ‚îÇ                    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ  Final Answer  ‚îÇ
                  ‚îÇ  + Sources     ‚îÇ
                  ‚îÇ  + Metadata    ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Types d'Intention

### 1. **general_conversation**
Questions qui ne n√©cessitent PAS de recherche dans la documentation OHADA.

**Exemples:**
- "Bonjour"
- "Qui es-tu ?"
- "Quel est ton r√¥le ?"
- "Merci beaucoup"
- "Comment √ßa va ?"

**Traitement:**
- Pas de recherche RAG
- R√©ponse directe du LLM avec system prompt
- Latence r√©duite
- `sources: []`

---

### 2. **rag_query**
Questions techniques n√©cessitant la documentation OHADA.

**Exemples:**
- "C'est quoi un amortissement ?"
- "Comment comptabiliser une cr√©ance douteuse ?"
- "Article 15 du SYSCOHADA"
- "Diff√©rence entre classe 2 et classe 3 ?"

**Traitement:**
- Recherche hybride (BM25 + Semantic)
- Reranking avec cross-encoder
- Context preparation
- LLM avec documentation
- Sources avec scores

---

### 3. **clarification**
Questions ambigu√´s n√©cessitant plus de contexte.

**Exemples:**
- "Qu'est-ce que c'est ?"
- "Peux-tu m'expliquer ?"
- "Et apr√®s ?"
- "Comment √ßa marche ?"

**Traitement:**
- Message demandant des pr√©cisions
- Pas de recherche RAG
- Guide l'utilisateur vers une question plus pr√©cise

---

## Composants Cl√©s

### 1. Intent Classifier Agent
**Fichier:** `src/rag/agents/intent_classifier.py`

```python
class IntentClassifierAgent:
    def __init__(self):
        # Utilise gpt-4o-mini pour classification rapide
        self.llm = ChatOpenAI(
            model=settings.llm_fallback_model,
            temperature=0  # D√©terministe
        )

        # Structured output pour fiabilit√©
        self.classifier = self.llm.with_structured_output(
            IntentClassification
        )

    async def classify_intent(self, query: str) -> IntentClassification:
        # Classification avec reasoning
        ...
```

**Avantages:**
- Classification pr√©cise via LLM
- Structured output (Pydantic)
- Raisonnement explicable
- Fallback automatique en cas d'erreur

---

### 2. RAG Workflow (LangGraph)
**Fichier:** `src/rag/agents/rag_workflow.py`

```python
class RAGWorkflow:
    def _build_graph(self) -> StateGraph:
        workflow = StateGraph(WorkflowState)

        # Nodes
        workflow.add_node("classify_intent", ...)
        workflow.add_node("direct_response", ...)
        workflow.add_node("retrieve_and_generate", ...)
        workflow.add_node("ask_clarification", ...)

        # Conditional routing
        workflow.add_conditional_edges(
            "classify_intent",
            self._route_by_intent,
            {...}
        )

        return workflow.compile()
```

**Avantages:**
- Graph-based orchestration
- Routing conditionnel
- State management
- Debuggable et tra√ßable

---

### 3. RAG Pipeline (Int√©gration)
**Fichier:** `src/rag/pipeline/rag_pipeline.py`

```python
class RAGPipeline:
    def __init__(self, use_workflow: bool = True):
        # use_workflow=True : Intent-based routing
        # use_workflow=False : Legacy direct pipeline

        if self.use_workflow:
            self.workflow = RAGWorkflow(rag_pipeline=self)

    async def query(self, query: str, ...) -> Dict[str, Any]:
        if self.use_workflow and self.workflow:
            # Nouveau syst√®me avec intent classification
            return await self.workflow.execute(...)
        else:
            # Legacy pipeline (fallback)
            ...
```

---

## D√©pendances

```txt
# LangGraph ecosystem
langgraph==0.2.60
langchain-openai==0.2.14
langchain==0.3.20
langchain-core==0.3.42
```

---

## Logs et Debugging

Le syst√®me produit des logs structur√©s √† chaque √©tape :

```json
{
  "event": "intent_classification_complete",
  "query": "Qui es-tu ?",
  "intent_type": "general_conversation",
  "confidence": 0.95,
  "reasoning": "Question sur l'identit√© de KAURI, ne n√©cessite pas de documentation"
}
```

```json
{
  "event": "workflow_routing",
  "intent_type": "rag_query",
  "confidence": 0.98
}
```

---

## Metadata dans la R√©ponse

Chaque r√©ponse contient maintenant :

```json
{
  "conversation_id": "uuid",
  "query": "...",
  "answer": "...",
  "sources": [...],
  "metadata": {
    "intent_type": "rag_query",
    "intent_confidence": 0.98,
    "intent_reasoning": "Question technique sur SYSCOHADA",
    "retrieval_time_ms": 245,
    "generation_time_ms": 1200,
    "num_sources": 5,
    "use_reranking": true,
    "model_used": "gpt-4o"
  }
}
```

---

## Avantages de l'Architecture

### üéØ Pr√©cision
- Classification LLM > patterns statiques
- Confidence scores pour monitoring
- Raisonnement explicable

### ‚ö° Performance
- Skip RAG pour questions g√©n√©rales
- Classification rapide (gpt-4o-mini)
- Latence optimis√©e

### üîß Maintenabilit√©
- Pas de patterns hardcod√©s
- Workflow visualisable
- Facile √† √©tendre (nouveaux intent types)

### üìä Observabilit√©
- Logs structur√©s
- Metadata riches
- Tra√ßabilit√© compl√®te

---

## √âvolutions Futures

### Phase 1 (Actuel)
‚úÖ Classification 3 types d'intention
‚úÖ Routing conditionnel
‚úÖ Int√©gration LangGraph

### Phase 2 (Possible)
- Multi-turn conversation support
- Intent history tracking
- Personalization par utilisateur

### Phase 3 (Avanc√©)
- Fine-tuning du classifier
- Multi-domain routing (comptabilit√©, juridique, fiscalit√©)
- Active learning from user feedback

---

## Configuration

Pour activer/d√©sactiver le workflow :

```python
# Dans src/rag/pipeline/rag_pipeline.py
pipeline = RAGPipeline(use_workflow=True)  # Nouveau syst√®me
pipeline = RAGPipeline(use_workflow=False)  # Legacy
```

Le syst√®me est con√ßu pour √™tre **backward-compatible** avec fallback automatique en cas d'erreur.

---

## Tests Recommand√©s

1. **Questions g√©n√©rales**
   - "Bonjour", "Qui es-tu ?", "Merci"
   - V√©rifier : `sources: []`, latence < 1s

2. **Questions OHADA**
   - "C'est quoi un amortissement ?"
   - V√©rifier : sources pr√©sentes, r√©f√©rences correctes

3. **Questions ambigu√´s**
   - "Qu'est-ce que c'est ?"
   - V√©rifier : message de clarification

4. **Edge cases**
   - Questions mixtes
   - Typos et variations
   - Questions multi-langues
