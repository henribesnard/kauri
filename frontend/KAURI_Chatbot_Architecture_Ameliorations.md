# KAURI Chatbot - Analyse d'Architecture et Recommandations d'Am√©lioration

> **Date**: 2025-11-04
> **Projet**: KAURI - Solution Comptable Intelligente OHADA
> **Service**: KAURI Chatbot (Microservice #13)
> **Version**: 2.0 (Migration depuis OHAD'AI)

---

## üìã Table des Mati√®res

1. [Vue d'Ensemble](#1-vue-densemble)
2. [Analyse de l'Architecture Actuelle](#2-analyse-de-larchitecture-actuelle)
3. [Points Forts](#3-points-forts)
4. [Points √† Am√©liorer](#4-points-√†-am√©liorer)
5. [Recommandations Architecturales](#5-recommandations-architecturales)
6. [Architecture Cible](#6-architecture-cible)
7. [Plan de Migration](#7-plan-de-migration)
8. [Roadmap d'Impl√©mentation](#8-roadmap-dimpl√©mentation)

---

## 1. Vue d'Ensemble

### 1.1 Contexte

Le chatbot KAURI est l'un des **25 microservices** de la plateforme KAURI. Il doit s'int√©grer dans une architecture microservices cloud-native tout en conservant ses capacit√©s RAG (Retrieval-Augmented Generation) expert en OHADA.

### 1.2 Architecture Actuelle (h√©rit√©e d'OHAD'AI)

**Type**: Application monolithique avec capacit√©s RAG  
**Stack**: FastAPI + ChromaDB + BGE-M3 + DeepSeek  
**D√©ploiement**: Serveur standalone sur port 8000  

---

## 2. Analyse de l'Architecture Actuelle

### 2.1 Stack Technique Actuelle

```yaml
Backend Framework: FastAPI 0.115.11
Base Vectorielle: ChromaDB 0.5.23 (locale, persist√©e sur disque)
Embeddings: BGE-M3 (local, 1024 dimensions)
LLM: DeepSeek-Chat (API) + OpenAI GPT-4 (fallback)
Reranking: Cross-Encoder (ms-marco-MiniLM-L-6-v2)
Cache: Redis 5.2.1 (optionnel, standalone)
Base SQL: SQLite (dev) / PostgreSQL (prod)
Serveur: Uvicorn 0.34.0
```

### 2.2 Points Forts ‚úÖ

#### 2.2.1 Recherche Hybride Sophistiqu√©e
```
‚úÖ BM25 (keyword) + Vector Search + Cross-Encoder Reranking
‚úÖ Parallel search execution
‚úÖ Intent classification avec LLM
‚úÖ Query reformulation
‚úÖ Context processing
```

**Forces**:
- Excellente qualit√© de recherche (NDCG@10 estim√© 0.70-0.75)
- Combinaison de plusieurs approches compl√©mentaires
- Reranking final pour optimiser la pertinence

#### 2.2.2 Optimisations Performance
```
‚úÖ Warm-up au startup (~200-500ms gagn√©s)
‚úÖ Cache Redis pour queries (95-98% gain latence)
‚úÖ Cache embeddings (50-150ms par embedding)
‚úÖ BGE-M3 local (vs API OpenAI)
‚úÖ Batch embeddings
‚úÖ Singleton pattern pour embedder
```

**Impact**: Latence 2-4s sans cache ‚Üí 0.05s avec cache

#### 2.2.3 Features Compl√®tes
```
‚úÖ Streaming SSE
‚úÖ Authentification JWT
‚úÖ Gestion conversations
‚úÖ Versioning documents
‚úÖ Metadata enrichment
‚úÖ Multi-collections (7 collections OHADA)
```

#### 2.2.4 Documentation Exhaustive
- Architecture d√©taill√©e
- Configuration LLM centralis√©e (YAML)
- Scripts d'ingestion document√©s
- Guide de reproduction complet

---

## 3. Points √† Am√©liorer ‚ö†Ô∏è

### 3.1 Architecture Microservices

#### üî¥ **Probl√®me Critique**: Monolithe dans un √âcosyst√®me Microservices

**√âtat actuel**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   OHAD'AI Backend (Monolithe)       ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  API Routes                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /query, /stream             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /auth, /conversations       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /documents                  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Retrieval System              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - BM25, Vector, Reranking     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Generation System             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Intent, Reformulation       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Response Generation         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Auth Manager                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - JWT, Password Utils         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì Port 8000
   [Client unique]
```

**Architecture cible KAURI**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         KAURI Ecosystem (25 services)        ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Auth Service ‚Üí User Service ‚Üí Chatbot       ‚îÇ
‚îÇ  Accounting ‚Üí Invoice ‚Üí Payment              ‚îÇ
‚îÇ  Document Service ‚Üí OCR Service              ‚îÇ
‚îÇ  Notification ‚Üí Workflow ‚Üí Analytics         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì API Gateway
   [Multiple clients]
```

**Impacts**:
- ‚ùå Couplage fort entre fonctionnalit√©s
- ‚ùå Auth r√©pliqu√©e (d√©j√† dans Auth Service)
- ‚ùå Pas de communication inter-services
- ‚ùå Scalabilit√© limit√©e (tout ou rien)
- ‚ùå D√©ploiement monolithique

**Recommandation**: D√©couper en **3 microservices** :
1. **Chatbot API Service** (orchestration, endpoints)
2. **RAG Engine Service** (recherche, embeddings)
3. **Knowledge Base Service** (gestion corpus, ingestion)

---

### 3.2 Base de Donn√©es Vectorielle

#### üü† **ChromaDB Local: Limites en Production**

**√âtat actuel**:
```python
# ChromaDB stock√© sur disque local
chroma_db/
‚îú‚îÄ‚îÄ chroma.sqlite3
‚îî‚îÄ‚îÄ [collections vectorielles]
```

**Probl√®mes**:
- ‚ùå Pas de scalabilit√© horizontale
- ‚ùå Pas de haute disponibilit√©
- ‚ùå Backup/restore complexe
- ‚ùå Pas de r√©plication multi-r√©gion
- ‚ùå Performances limit√©es (disque local)
- ‚ùå Pas adapt√© au cloud distribu√©

**Alternatives recommand√©es**:

| Solution | Avantages | Inconv√©nients | Recommandation |
|----------|-----------|---------------|----------------|
| **Pinecone** | Managed, HA, scalable, performant | Co√ªt √©lev√©, vendor lock-in | ‚úÖ **Id√©al production** |
| **Qdrant** | Open-source, scalable, performant | Self-hosted | ‚úÖ Bon compromis |
| **Weaviate** | Open-source, GraphQL, hybrid search | Plus complexe | ‚ö†Ô∏è Si GraphQL utile |
| **Milvus** | Open-source, tr√®s performant, features avanc√©es | Infrastructure lourde | ‚ö†Ô∏è Si gros volumes |
| **ChromaDB** | Simple, l√©ger | Pas pour production scale | ‚ùå Dev/test uniquement |

**Choix recommand√©**: **Pinecone** (production) + **Qdrant** (alternative self-hosted)

**Migration estim√©e**: 2-3 semaines

---

### 3.3 Base de Donn√©es Relationnelle

#### üü† **SQLite: Non adapt√© √† la production**

**√âtat actuel**:
```python
# Backend utilise SQLite en dev/test
data/
‚îî‚îÄ‚îÄ ohada_users.db  # SQLite
```

**Probl√®mes**:
- ‚ùå Pas de concurrence
- ‚ùå Pas de scalabilit√©
- ‚ùå Pas de r√©plication
- ‚ùå Limites transactions

**Recommandation**: 
- ‚úÖ PostgreSQL d√©j√† pr√©vu dans sp√©cifications
- ‚úÖ Migrations SQLAlchemy en place
- ‚ö†Ô∏è V√©rifier que tout fonctionne bien avec PostgreSQL

**Action**: Basculer d√©finitivement sur PostgreSQL (d√©j√† configur√© dans docker-compose)

---

### 3.4 S√©curit√©

#### üü° **Manques Critiques**

**√âtat actuel**:
```python
# JWT optionnel
# Pas de rate limiting
# Pas de validation d'entr√©es stricte
# Pas de protection DDoS
# Secrets en environnement simple
```

**Recommandations**:

1. **Rate Limiting**
```python
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter

@app.post("/query", dependencies=[Depends(RateLimiter(times=10, seconds=60))])
```

2. **Input Validation**
```python
from pydantic import BaseModel, Field, validator

class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    
    @validator('query')
    def sanitize_query(cls, v):
        # Nettoyer injection, XSS, etc.
        return v.strip()
```

3. **Secrets Management**
- ‚úÖ Utiliser AWS Secrets Manager / HashiCorp Vault
- ‚ùå Pas de secrets dans .env en production

4. **CORS Strict**
```python
# Actuellement trop permissif
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ‚ùå Dangereux
    allow_methods=["*"],
)

# Recommand√©
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,  # Liste blanche
    allow_methods=["GET", "POST"],
    allow_headers=["Authorization", "Content-Type"],
)
```

5. **Auth avec Auth Service**
- D√©l√©guer authentification au Auth Service (SSO)
- Token validation via API Gateway

---

### 3.5 Observabilit√©

#### üü° **Monitoring Insuffisant**

**√âtat actuel**:
```python
# Logs basiques
# Pas de m√©triques structur√©es
# Pas de tracing distribu√©
# Pas d'alertes
```

**Recommandations**:

1. **Structured Logging**
```python
import structlog

logger = structlog.get_logger()

logger.info("query_received", 
    user_id=user_id,
    query_length=len(query),
    intent=intent,
    duration_ms=duration
)
```

2. **M√©triques Prometheus**
```python
from prometheus_fastapi_instrumentator import Instrumentator

Instrumentator().instrument(app).expose(app)

# M√©triques custom
query_duration = Histogram('query_duration_seconds', 'Query processing time')
query_errors = Counter('query_errors_total', 'Total query errors')
```

3. **Distributed Tracing (Jaeger)**
```python
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

tracer = trace.get_tracer(__name__)

@app.post("/query")
async def query_endpoint(request: QueryRequest):
    with tracer.start_as_current_span("process_query"):
        # Trace chaque √©tape
        with tracer.start_as_current_span("embed_query"):
            embedding = await generate_embedding(query)
        with tracer.start_as_current_span("search_documents"):
            results = await search(embedding)
```

4. **Health Checks Avanc√©s**
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "checks": {
            "database": check_db(),
            "vector_db": check_vector_db(),
            "redis": check_redis(),
            "llm_api": check_llm_api(),
            "embedder": check_embedder()
        }
    }
```

5. **Alerting (Grafana + Prometheus)**
- Alertes sur latence > 5s
- Alertes sur taux d'erreur > 5%
- Alertes sur disponibilit√© < 99%

---

### 3.6 Communication Inter-Services

#### üî¥ **Pas d'Event-Driven Architecture**

**√âtat actuel**:
- Communication synchrone uniquement (REST)
- Pas de message bus
- Couplage temporel fort

**Recommandations**:

1. **Event Bus (Kafka ou RabbitMQ)**
```yaml
Events √† publier:
  - chatbot.query.received
  - chatbot.query.completed
  - chatbot.error.occurred
  - knowledge.document.added
  - knowledge.document.updated

Events √† consommer:
  - user.created (du User Service)
  - accounting.entry.created (pour enrichir contexte)
  - document.uploaded (du Document Service)
```

2. **Async Processing (Celery d√©j√† en place)**
```python
# Bien: ingestion asynchrone
@celery.task
def ingest_document(document_id):
    # T√¢che longue en arri√®re-plan
    pass
```

3. **Saga Pattern pour Transactions Distribu√©es**
```python
# Si cr√©ation conversation n√©cessite plusieurs services
async def create_conversation_saga(user_id, query):
    # 1. Cr√©er conversation
    conversation = await conversation_service.create(user_id)
    
    # 2. Publier event
    await event_bus.publish("chatbot.conversation.created", conversation)
    
    # 3. Traiter query
    try:
        response = await process_query(query)
    except Exception as e:
        # Rollback: supprimer conversation
        await conversation_service.delete(conversation.id)
        raise
```

---

### 3.7 Configuration et Environnements

#### üü° **Config YAML Bien Mais Peut √ätre Mieux**

**√âtat actuel**:
```yaml
# llm_config_test.yaml et llm_config_production.yaml
# Bien: s√©paration environnements
# Manque: gestion centralis√©e multi-services
```

**Recommandations**:

1. **Config Service Centralis√© (Consul ou Spring Cloud Config)**
```
Config Service
    ‚Üì
[Chatbot] [Auth] [Accounting] ...
```

2. **Feature Flags (LaunchDarkly ou Unleash)**
```python
if feature_flags.is_enabled("use_gpt4_for_complex_queries"):
    llm = "gpt-4"
else:
    llm = "deepseek"
```

3. **Secrets Rotation**
- Rotation automatique des API keys
- Alertes sur secrets expirant

---

### 3.8 Testing

#### üü° **Tests Limit√©s**

**√âtat actuel**:
```bash
# Tests unitaires mentionn√©s mais pas d√©taill√©s
pytest tests/ -v
```

**Recommandations**:

1. **Test Pyramid**
```
       /\
      /  \  E2E Tests (10%)
     /    \
    / Inte \  Integration Tests (30%)
   /  gration\
  /    Tests  \
 /              \
/________________\  Unit Tests (60%)
```

2. **Tests Sp√©cifiques RAG**
```python
def test_hybrid_search_quality():
    """Test NDCG@10 > 0.70"""
    queries = load_test_queries()
    results = [retriever.search(q) for q in queries]
    ndcg = calculate_ndcg(results)
    assert ndcg > 0.70

def test_response_quality():
    """Test r√©ponses avec ground truth"""
    qa_pairs = load_qa_dataset()
    for question, expected in qa_pairs:
        response = chatbot.query(question)
        similarity = compute_similarity(response, expected)
        assert similarity > 0.80
```

3. **Load Testing**
```bash
# Tester 100 req/s pendant 10 minutes
locust -f tests/load_test.py --users 100 --spawn-rate 10 --run-time 10m
```

4. **Regression Testing**
- Tester automatiquement √† chaque d√©ploiement
- Dataset de questions/r√©ponses de r√©f√©rence

---

### 3.9 D√©ploiement

#### üü° **Pas de CI/CD Ni Containerisation Mentionn√©e**

**√âtat actuel**:
```bash
# Scripts .bat pour Windows
# Pas de Docker pour le code (uniquement services)
```

**Recommandations**:

1. **Dockerization**
```dockerfile
# Dockerfile.chatbot
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Download BGE-M3 model at build time
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('BAAI/bge-m3')"

# Copy application
COPY src/ ./src/

# Healthcheck
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s \
  CMD curl -f http://localhost:8000/health || exit 1

# Run
CMD ["uvicorn", "src.api.ohada_api_server:app", "--host", "0.0.0.0", "--port", "8000"]
```

2. **Kubernetes Deployment**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kauri-chatbot
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kauri-chatbot
  template:
    metadata:
      labels:
        app: kauri-chatbot
    spec:
      containers:
      - name: chatbot
        image: kauri/chatbot:1.0.0
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: chatbot-secrets
              key: database-url
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
```

3. **CI/CD Pipeline (GitHub Actions)**
```yaml
name: Chatbot CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Run tests
      run: |
        pip install -r requirements.txt
        pytest tests/ --cov=src --cov-report=xml
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Build Docker image
      run: docker build -t kauri/chatbot:${{ github.sha }} .
    - name: Push to registry
      run: docker push kauri/chatbot:${{ github.sha }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Deploy to Kubernetes
      run: |
        kubectl set image deployment/kauri-chatbot \
          chatbot=kauri/chatbot:${{ github.sha }}
```

4. **Helm Charts**
```yaml
# values.yaml
replicaCount: 3

image:
  repository: kauri/chatbot
  tag: "1.0.0"

resources:
  requests:
    memory: "2Gi"
    cpu: "1000m"
  limits:
    memory: "4Gi"
    cpu: "2000m"

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
```

---

### 3.10 Documentation

#### ‚úÖ **Excellent mais Peut √ätre Enrichi**

**Points forts**:
- Architecture d√©taill√©e
- Configuration exhaustive
- Scripts document√©s

**Am√©liorations**:
1. **API Documentation (OpenAPI/Swagger)**
```python
@app.post("/query", 
    summary="Query the chatbot",
    description="Send a query and get AI-powered response with OHADA sources",
    response_model=QueryResponse,
    responses={
        200: {"description": "Successful response"},
        400: {"description": "Invalid query"},
        429: {"description": "Rate limit exceeded"},
        500: {"description": "Internal server error"}
    }
)
```

2. **ADRs (Architecture Decision Records)**
```markdown
# ADR-001: Choice of Vector Database

Date: 2025-11-03
Status: Accepted

## Context
Need scalable vector database for production.

## Decision
Use Pinecone for production deployments.

## Consequences
+ Managed service, high availability
+ Excellent performance
- Cost ~$70/month for 1M vectors
```

3. **Runbooks**
```markdown
# Runbook: High Latency on /query Endpoint

## Symptoms
- Response time > 5s
- User complaints

## Investigation
1. Check Grafana dashboard
2. Check vector DB latency
3. Check LLM API status
4. Check cache hit rate

## Resolution
1. If LLM API down ‚Üí switch to fallback
2. If cache hit rate low ‚Üí warm cache
3. If vector DB slow ‚Üí check indices
```

---

## 4. Architecture Cible Propos√©e

### 4.1 Vue d'Ensemble Microservices

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        API Gateway (Kong)                        ‚îÇ
‚îÇ  - Rate Limiting, Auth, Routing, Load Balancing                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                ‚îÇ                ‚îÇ
        ‚ñº                ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Chatbot API  ‚îÇ  ‚îÇ RAG Engine   ‚îÇ  ‚îÇ Knowledge    ‚îÇ
‚îÇ   Service    ‚îÇ  ‚îÇ   Service    ‚îÇ  ‚îÇBase Service  ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ - REST API   ‚îÇ  ‚îÇ - Embeddings ‚îÇ  ‚îÇ - Ingestion  ‚îÇ
‚îÇ - Streaming  ‚îÇ  ‚îÇ - Search     ‚îÇ  ‚îÇ - Corpus Mgt ‚îÇ
‚îÇ - Validation ‚îÇ  ‚îÇ - Reranking  ‚îÇ  ‚îÇ - Versioning ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                 ‚îÇ                 ‚îÇ
       ‚îÇ                 ‚îÇ                 ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                ‚îÇ                        ‚îÇ
        ‚ñº                ‚ñº                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Auth Service ‚îÇ  ‚îÇ User Service ‚îÇ  ‚îÇ Document Service ‚îÇ
‚îÇ (External)   ‚îÇ  ‚îÇ (External)   ‚îÇ  ‚îÇ (External)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                ‚îÇ                        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ    Event Bus (Kafka)   ‚îÇ
            ‚îÇ  - chatbot.*           ‚îÇ
            ‚îÇ  - knowledge.*         ‚îÇ
            ‚îÇ  - user.*              ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 Service #1: Chatbot API Service

**Responsabilit√©s**:
- Exposer API REST pour le chatbot
- G√©rer conversations et sessions
- Orchestrer appels aux autres services
- Streaming SSE
- Rate limiting, validation

**Stack**:
```yaml
Framework: FastAPI
Language: Python 3.11+
Dependencies:
  - RAG Engine Service (gRPC)
  - Auth Service (REST)
  - User Service (REST)
  - Event Bus (Kafka producer)
```

**Endpoints**:
```
POST   /v1/chat/query              # Query synchrone
GET    /v1/chat/stream             # Query streaming SSE
GET    /v1/chat/conversations      # Liste conversations
POST   /v1/chat/conversations      # Cr√©er conversation
GET    /v1/chat/conversations/:id  # D√©tails conversation
DELETE /v1/chat/conversations/:id  # Supprimer conversation
GET    /v1/health                  # Health check
GET    /v1/metrics                 # Prometheus metrics
```

**Scaling**: 3-10 replicas selon charge

---

### 4.3 Service #2: RAG Engine Service

**Responsabilit√©s**:
- Recherche hybride (BM25 + Vector)
- G√©n√©ration embeddings (BGE-M3)
- Reranking (Cross-Encoder)
- Appel LLM (DeepSeek, GPT-4)
- Cache r√©sultats

**Stack**:
```yaml
Framework: FastAPI (ou gRPC pour performance)
Language: Python 3.11+
Vector DB: Pinecone (prod) / Qdrant (self-hosted)
Cache: Redis Cluster
Models:
  - BGE-M3 (embeddings)
  - Cross-Encoder (reranking)
  - DeepSeek API (LLM)
```

**API interne (gRPC)**:
```protobuf
service RAGEngine {
  rpc Search(SearchRequest) returns (SearchResponse);
  rpc GenerateResponse(GenerateRequest) returns (stream GenerateResponse);
  rpc GenerateEmbedding(EmbeddingRequest) returns (EmbeddingResponse);
}
```

**Scaling**: 2-5 replicas (CPU/GPU-intensive)

---

### 4.4 Service #3: Knowledge Base Service

**Responsabilit√©s**:
- Ingestion documents (batch/streaming)
- Parsing (Word, PDF, etc.)
- Chunking et preprocessing
- Enrichissement m√©tadonn√©es
- Versioning corpus
- Synchronisation avec Pinecone

**Stack**:
```yaml
Framework: FastAPI
Language: Python 3.11+
Database: PostgreSQL (metadata)
Object Storage: AWS S3 / MinIO (documents bruts)
Queue: Celery + Redis (async tasks)
Vector DB: Pinecone (sync)
```

**Endpoints**:
```
POST /v1/knowledge/documents          # Ingest single doc
POST /v1/knowledge/documents/batch    # Ingest batch
GET  /v1/knowledge/documents          # Liste documents
GET  /v1/knowledge/documents/:id      # D√©tails document
PUT  /v1/knowledge/documents/:id      # Update document
DELETE /v1/knowledge/documents/:id    # Delete document
GET  /v1/knowledge/stats              # Statistiques corpus
POST /v1/knowledge/sync               # Force sync avec Pinecone
```

**Scaling**: 1-3 replicas (IO-intensive)

---

### 4.5 Infrastructure Partag√©e

#### 4.5.1 Bases de Donn√©es

```yaml
PostgreSQL:
  Type: AWS RDS / GCP Cloud SQL
  R√©plication: Multi-AZ
  Backup: Daily automated
  Connexions: PgBouncer pool
  Usage:
    - Chatbot: conversations, messages
    - Knowledge: documents metadata, versions

Pinecone:
  Type: Managed Vector DB
  Index: Standard (1024 dim)
  Pods: p1 ou s1 selon volume
  Regions: Multi-region replication
  Usage: Embeddings OHADA corpus

Redis Cluster:
  Type: AWS ElastiCache / Redis Enterprise
  Nodes: 3+ nodes (HA)
  Persistence: AOF + RDB
  Usage:
    - Cache queries/embeddings
    - Celery broker
    - Rate limiting
```

#### 4.5.2 Message Bus

```yaml
Kafka:
  Type: Confluent Cloud / AWS MSK
  Topics:
    - chatbot.query.received
    - chatbot.query.completed
    - chatbot.error
    - knowledge.document.added
    - knowledge.document.updated
    - user.created (from User Service)
  Retention: 7 days
  Partitions: 3-10 par topic
```

#### 4.5.3 Observabilit√©

```yaml
Logging:
  Stack: ELK (Elasticsearch + Logstash + Kibana)
  Format: JSON structured logs
  Retention: 30 days

Metrics:
  Stack: Prometheus + Grafana
  Exporters: Per-service /metrics endpoint
  Alerts: PagerDuty integration

Tracing:
  Stack: Jaeger
  Sampling: 10% in prod, 100% in staging
```

---

## 5. Comparaison Architecture Actuelle vs Cible

| Aspect | Actuel | Cible | B√©n√©fices |
|--------|--------|-------|-----------|
| **Architecture** | Monolithe | 3 microservices | Scalabilit√© ind√©pendante, isolation des pannes |
| **Vector DB** | ChromaDB local | Pinecone managed | HA, performance, r√©plication |
| **SQL DB** | SQLite | PostgreSQL (RDS) | Concurrence, r√©plication, backup |
| **Cache** | Redis standalone | Redis Cluster | HA, sharding |
| **Auth** | Interne (JWT) | Auth Service (SSO) | Centralisation, s√©curit√© |
| **Communication** | REST sync | REST + gRPC + Events | Performance, d√©couplage |
| **D√©ploiement** | Scripts .bat | Kubernetes + Helm | Automation, orchestration |
| **Monitoring** | Logs basiques | ELK + Prometheus + Jaeger | Observabilit√© compl√®te |
| **CI/CD** | Aucun | GitHub Actions | D√©ploiement automatis√© |
| **S√©curit√©** | Basique | Rate limiting + Secrets Mgmt | Protection DDoS, secrets s√©curis√©s |
| **Testing** | Minimal | Pyramid (unit/integration/e2e) | Qualit√©, non-r√©gression |

---

## 6. Plan de Migration

### Phase 1: Pr√©paration (2 semaines)

**Objectifs**:
- Finaliser sp√©cifications d√©taill√©es
- Setup infrastructure cible
- Pr√©parer environnements

**T√¢ches**:
1. ‚úÖ Provisionner PostgreSQL (RDS)
2. ‚úÖ Provisionner Redis Cluster (ElastiCache)
3. ‚úÖ Cr√©er compte Pinecone et setup index
4. ‚úÖ Setup Kubernetes cluster (EKS / GKE / AKS)
5. ‚úÖ Configurer API Gateway (Kong)
6. ‚úÖ Setup Kafka (MSK / Confluent)
7. ‚úÖ Setup observabilit√© (ELK + Prometheus + Jaeger)
8. ‚úÖ Cr√©er repos Git s√©par√©s pour chaque service

---

### Phase 2: Migration Backend (6 semaines)

#### Semaine 1-2: D√©coupage Monolithe

**Objectif**: Extraire les 3 microservices

**T√¢ches**:
1. Cr√©er structure projet `kauri-chatbot-api/`
   ```
   kauri-chatbot-api/
   ‚îú‚îÄ‚îÄ src/
   ‚îÇ   ‚îú‚îÄ‚îÄ api/
   ‚îÇ   ‚îú‚îÄ‚îÄ models/
   ‚îÇ   ‚îú‚îÄ‚îÄ services/
   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
   ‚îú‚îÄ‚îÄ tests/
   ‚îú‚îÄ‚îÄ Dockerfile
   ‚îú‚îÄ‚îÄ requirements.txt
   ‚îî‚îÄ‚îÄ helm/
   ```

2. Extraire endpoints API uniquement
3. Remplacer auth interne par appels Auth Service
4. Impl√©menter gRPC client vers RAG Engine

5. Cr√©er structure projet `kauri-rag-engine/`
   ```
   kauri-rag-engine/
   ‚îú‚îÄ‚îÄ src/
   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval/
   ‚îÇ   ‚îú‚îÄ‚îÄ generation/
   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
   ‚îÇ   ‚îî‚îÄ‚îÄ cache/
   ‚îú‚îÄ‚îÄ proto/
   ‚îú‚îÄ‚îÄ tests/
   ‚îú‚îÄ‚îÄ Dockerfile
   ‚îî‚îÄ‚îÄ requirements.txt
   ```

6. Migrer syst√®me de recherche hybride
7. Impl√©menter serveur gRPC
8. Setup cache Redis Cluster

9. Cr√©er structure projet `kauri-knowledge-base/`
   ```
   kauri-knowledge-base/
   ‚îú‚îÄ‚îÄ src/
   ‚îÇ   ‚îú‚îÄ‚îÄ ingest/
   ‚îÇ   ‚îú‚îÄ‚îÄ parsers/
   ‚îÇ   ‚îú‚îÄ‚îÄ sync/
   ‚îÇ   ‚îî‚îÄ‚îÄ api/
   ‚îú‚îÄ‚îÄ celery_worker/
   ‚îú‚îÄ‚îÄ tests/
   ‚îú‚îÄ‚îÄ Dockerfile
   ‚îî‚îÄ‚îÄ requirements.txt
   ```

10. Migrer scripts d'ingestion
11. Impl√©menter API REST
12. Setup Celery workers

#### Semaine 3: Migration Vector DB

**Objectif**: Migrer ChromaDB ‚Üí Pinecone

**T√¢ches**:
1. Export donn√©es ChromaDB
   ```python
   from src.vector_db.ohada_vector_db_structure import OhadaVectorDB
   
   db = OhadaVectorDB()
   all_collections = db.get_all_collections()
   
   for collection_name in all_collections:
       collection = db.client.get_collection(collection_name)
       data = collection.get(include=["embeddings", "documents", "metadatas"])
       
       # Export to JSON
       with open(f"{collection_name}_export.json", "w") as f:
           json.dump(data, f)
   ```

2. Import vers Pinecone
   ```python
   import pinecone
   
   pinecone.init(api_key="...", environment="...")
   
   # Cr√©er index (1024 dimensions pour BGE-M3)
   pinecone.create_index("kauri-ohada", dimension=1024, metric="cosine")
   index = pinecone.Index("kauri-ohada")
   
   # Batch upsert
   for collection_name in all_collections:
       with open(f"{collection_name}_export.json") as f:
           data = json.load(f)
       
       vectors = [
           (id, embedding, metadata)
           for id, embedding, metadata in zip(data["ids"], data["embeddings"], data["metadatas"])
       ]
       
       # Upsert par batches de 100
       for i in range(0, len(vectors), 100):
           batch = vectors[i:i+100]
           index.upsert(vectors=batch, namespace=collection_name)
   ```

3. Tester recherche Pinecone vs ChromaDB (qualit√© identique)
4. Basculer code sur Pinecone
5. V√©rifier performances

#### Semaine 4: Containerisation

**Objectif**: Dockeriser les 3 services

**T√¢ches**:
1. √âcrire Dockerfiles optimis√©s (multi-stage builds)
2. Setup CI pour build images
3. Push vers registry (AWS ECR / GCP GCR)
4. Tester images localement

#### Semaine 5: D√©ploiement Kubernetes

**Objectif**: D√©ployer sur Kubernetes

**T√¢ches**:
1. √âcrire Helm charts
2. Configurer secrets (Sealed Secrets)
3. Setup Ingress (API Gateway)
4. D√©ployer sur cluster staging
5. Tests end-to-end

#### Semaine 6: Observabilit√© & Monitoring

**Objectif**: Setup monitoring complet

**T√¢ches**:
1. Int√©grer structured logging
2. Exposer m√©triques Prometheus
3. Configurer Jaeger tracing
4. Cr√©er dashboards Grafana
5. Setup alertes

---

### Phase 3: Migration Base de Donn√©es (1 semaine)

**Objectif**: Migrer SQLite ‚Üí PostgreSQL

**T√¢ches**:
1. Export donn√©es SQLite
   ```bash
   sqlite3 data/ohada_users.db .dump > dump.sql
   ```

2. Adapter SQL pour PostgreSQL
   ```bash
   # Remplacer syntaxe SQLite par PostgreSQL
   sed -i 's/AUTOINCREMENT/SERIAL/g' dump.sql
   ```

3. Import vers PostgreSQL
   ```bash
   psql -U ohada_user -d ohada -f dump.sql
   ```

4. V√©rifier int√©grit√© donn√©es
5. Basculer `DATABASE_URL` sur PostgreSQL

---

### Phase 4: Tests & Validation (2 semaines)

#### Semaine 1: Tests Fonctionnels

**T√¢ches**:
1. Tests unitaires (coverage > 80%)
2. Tests d'int√©gration inter-services
3. Tests end-to-end
4. Tests de r√©gression (qualit√© r√©ponses)

#### Semaine 2: Tests Non-Fonctionnels

**T√¢ches**:
1. Load testing (1000 req/s pendant 10 min)
2. Stress testing (jusqu'√† rupture)
3. Tests de r√©silience (chaos engineering)
4. Tests de s√©curit√© (OWASP)

---

### Phase 5: D√©ploiement Production (1 semaine)

**Objectif**: Mise en production avec Blue/Green

**T√¢ches**:
1. D√©ploiement environnement Blue (nouveau)
2. Smoke tests sur Blue
3. Basculer 10% trafic sur Blue (canary)
4. Monitorer m√©triques 24h
5. Si OK ‚Üí Basculer 100% trafic sur Blue
6. Si KO ‚Üí Rollback sur Green
7. Apr√®s validation ‚Üí Supprimer Green

---

## 7. Roadmap d'Impl√©mentation

### Timeline Global: 12 semaines

```
Semaines 1-2:   [===== Pr√©paration =====]
Semaines 3-8:   [========== Migration Backend ==========]
Semaine 9:      [= Migration DB =]
Semaines 10-11: [====== Tests ======]
Semaine 12:     [== Prod ==]
```

### Priorit√©s

**P0 - Critique (blocker production)**:
- ‚úÖ Migration Vector DB (ChromaDB ‚Üí Pinecone)
- ‚úÖ Migration SQL DB (SQLite ‚Üí PostgreSQL)
- ‚úÖ S√©curit√© (rate limiting, input validation)
- ‚úÖ Monitoring (logs, m√©triques, alertes)

**P1 - Important (qualit√© production)**:
- ‚úÖ D√©coupage microservices (3 services)
- ‚úÖ Containerisation + Kubernetes
- ‚úÖ CI/CD pipeline
- ‚úÖ Event-driven communication (Kafka)

**P2 - Nice to have (optimisations)**:
- ‚ö†Ô∏è gRPC pour communication inter-services (vs REST)
- ‚ö†Ô∏è Service mesh (Istio) pour traffic management
- ‚ö†Ô∏è GitOps (ArgoCD) pour d√©ploiements
- ‚ö†Ô∏è API versioning strict (v2, v3, etc.)

---

## 8. Risques et Mitigation

### Risque 1: Migration Vector DB Perd en Qualit√©

**Impact**: R√©ponses moins pertinentes  
**Probabilit√©**: Faible (Pinecone > ChromaDB)  
**Mitigation**:
- Tests A/B ChromaDB vs Pinecone avant migration
- Garder backup ChromaDB pendant 1 mois
- Rollback facile si probl√®me

### Risque 2: Latence Augmente (Communication Inter-Services)

**Impact**: UX d√©grad√©e  
**Probabilit√©**: Moyenne  
**Mitigation**:
- Utiliser gRPC (plus rapide que REST)
- Cache agressif (Redis)
- Async processing (√©v√©nements)
- Load testing avant prod

### Risque 3: Complexit√© Op√©rationnelle

**Impact**: Difficult√© maintenance  
**Probabilit√©**: √âlev√©e  
**Mitigation**:
- Documentation exhaustive
- Runbooks pour incidents courants
- Formation √©quipe DevOps
- Observabilit√© compl√®te (logs, metrics, tracing)

### Risque 4: Co√ªts Infrastructure

**Impact**: Budget d√©pass√©  
**Probabilit√©**: Moyenne  
**Mitigation**:
- Estimation co√ªts √† l'avance:
  - Pinecone: ~$70/mois (1M vectors)
  - RDS PostgreSQL: ~$100/mois
  - ElastiCache Redis: ~$50/mois
  - Kubernetes: ~$150/mois
  - **Total: ~$370/mois** (estim√©)
- Monitoring co√ªts (AWS Cost Explorer)
- Auto-scaling pour optimiser

---

## 9. M√©triques de Succ√®s

### 9.1 Performance

| M√©trique | Actuel | Cible | Mesure |
|----------|--------|-------|--------|
| Latence p50 (sans cache) | 2-4s | < 2s | Prometheus |
| Latence p95 (sans cache) | 4-6s | < 3s | Prometheus |
| Latence p50 (avec cache) | 50ms | < 100ms | Prometheus |
| Throughput | 10 req/s | 100 req/s | Load testing |

### 9.2 Qualit√©

| M√©trique | Actuel | Cible | Mesure |
|----------|--------|-------|--------|
| NDCG@10 | 0.70-0.75 | > 0.75 | Tests r√©gression |
| Precision@5 | Non mesur√© | > 80% | Tests r√©gression |
| Recall@10 | Non mesur√© | > 70% | Tests r√©gression |

### 9.3 Disponibilit√©

| M√©trique | Actuel | Cible | Mesure |
|----------|--------|-------|--------|
| Uptime | Non mesur√© | 99.9% | Prometheus |
| MTTR | Non mesur√© | < 5 min | Incidents |

### 9.4 √âvolutivit√©

| M√©trique | Actuel | Cible | Mesure |
|----------|--------|-------|--------|
| Scalabilit√© | Verticale uniquement | Horizontale | Kubernetes HPA |
| Replicas | 1 | 2-10 (auto) | Kubernetes |
| Multi-r√©gion | Non | Oui (3 r√©gions) | Infrastructure |

---

## 10. Conclusion

### R√©sum√© des Am√©liorations Cl√©s

1. **Architecture Microservices**: D√©coupage en 3 services autonomes
2. **Vector DB Production-Ready**: Pinecone avec HA et r√©plication
3. **SQL DB Robuste**: PostgreSQL avec r√©plication multi-AZ
4. **S√©curit√© Renforc√©e**: Rate limiting, input validation, secrets management
5. **Observabilit√© Compl√®te**: Logs structur√©s, m√©triques, tracing distribu√©
6. **CI/CD Automatis√©**: GitHub Actions + Kubernetes + Helm
7. **Event-Driven**: Communication asynchrone via Kafka
8. **Scalabilit√©**: Auto-scaling horizontal Kubernetes

### Gains Attendus

| Aspect | Gain |
|--------|------|
| **Disponibilit√©** | 99.5% ‚Üí 99.9% |
| **Scalabilit√©** | 10 req/s ‚Üí 100 req/s |
| **Latence** | -30% (gr√¢ce √† Pinecone + gRPC) |
| **Maintenabilit√©** | +50% (gr√¢ce √† microservices) |
| **Time to Market** | -40% (gr√¢ce √† CI/CD) |
| **Co√ªts Ops** | -20% (gr√¢ce √† monitoring) |

### Prochaines √âtapes Imm√©diates

1. ‚úÖ Valider architecture cible avec √©quipe
2. ‚úÖ Estimer co√ªts infrastructure (Pinecone + RDS + K8s)
3. ‚úÖ Provisionner infrastructure cible
4. ‚úÖ D√©marrer Phase 1 (Pr√©paration)
5. ‚úÖ Recruter/former √©quipe DevOps si n√©cessaire

---

**Date de cr√©ation**: 2025-11-04
**Version**: 2.0
**Auteur**: Architecture Team
**Statut**: Proposition - En attente de validation

---

## Annexes

### A. Comparatif Vector Databases

| Feature | ChromaDB | Pinecone | Qdrant | Weaviate | Milvus |
|---------|----------|----------|--------|----------|--------|
| **Type** | Embedded | Managed | Self/Managed | Self/Managed | Self-hosted |
| **Scalabilit√©** | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **HA** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Performance** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Co√ªt** | Free | $$$ | $$ | $$ | $$ |
| **Facilit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **API** | Python | REST/gRPC | REST/gRPC | GraphQL/REST | Python/gRPC |

**Recommandation**: Pinecone (prod) ou Qdrant (self-hosted)

### B. Estimation Co√ªts Mensuelle

```
Infrastructure:
  - Pinecone (1M vectors, p1):     $70
  - RDS PostgreSQL (db.t3.medium): $100
  - ElastiCache Redis (3 nodes):   $50
  - Kubernetes (3 nodes t3.large): $150
  - S3 (documents storage):        $10
  - CloudWatch (logs):             $20
  - Data Transfer:                 $30
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTAL:                           ~$430/mois

Personnel (estim√©):
  - DevOps Engineer:               $5000/mois
  - Backend Engineer:              $4000/mois
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTAL:                           $9000/mois

Total Mensuel:                     ~$9430/mois
```

### C. Glossaire

- **RAG**: Retrieval-Augmented Generation
- **BM25**: Okapi Best Matching 25 (algorithme de recherche keyword)
- **BGE-M3**: BAAI General Embedding Model M3
- **Cross-Encoder**: Mod√®le de reranking
- **NDCG**: Normalized Discounted Cumulative Gain
- **SSE**: Server-Sent Events
- **HPA**: Horizontal Pod Autoscaler (Kubernetes)
- **RDS**: Relational Database Service (AWS)
- **ELK**: Elasticsearch, Logstash, Kibana
