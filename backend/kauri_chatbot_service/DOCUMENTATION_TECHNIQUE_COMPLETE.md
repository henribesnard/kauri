# ğŸ“š Documentation Technique ComplÃ¨te - KAURI Chatbot Service

**Version:** 2.1.0
**Date:** 08/11/2025
**Service:** kauri_chatbot_service
**Port:** 3202

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'Ensemble](#vue-densemble)
2. [Architecture GÃ©nÃ©rale](#architecture-gÃ©nÃ©rale)
3. [Pipeline RAG Conversationnel](#pipeline-rag-conversationnel)
4. [Workflow LangGraph](#workflow-langgraph)
5. [Classification d'Intent](#classification-dintent)
6. [Retrieval Hybride](#retrieval-hybride)
7. [MÃ©tadonnÃ©es Enrichies](#mÃ©tadonnÃ©es-enrichies)
8. [Endpoints API](#endpoints-api)
9. [Configuration](#configuration)
10. [Base de DonnÃ©es](#base-de-donnÃ©es)
11. [Flux de DonnÃ©es](#flux-de-donnÃ©es)
12. [Fichiers ClÃ©s](#fichiers-clÃ©s)

---

## ğŸ¯ Vue d'Ensemble

### Objectif

KAURI Chatbot Service est un systÃ¨me RAG (Retrieval-Augmented Generation) conversationnel spÃ©cialisÃ© en **droit et comptabilitÃ© OHADA**. Il permet aux juristes et comptables d'interroger une base documentaire structurÃ©e (Actes Uniformes, Plan Comptable, Jurisprudences, Doctrines) via une interface conversationnelle intelligente.

### SpÃ©cificitÃ©s OHADA

- **Domaine juridique** : Droit des affaires harmonisÃ© (17 Ã‰tats membres)
- **Domaine comptable** : SYSCOHADA (SystÃ¨me Comptable OHADA)
- **Types de documents** : Actes Uniformes, Plan Comptable, Jurisprudences CCJA, Doctrines
- **RÃ©fÃ©rences prÃ©cises** : Articles, Comptes, Chapitres, Juridictions

### FonctionnalitÃ©s Principales

1. **RAG Conversationnel** : RÃ©ponses basÃ©es sur documents avec historique
2. **Classification d'Intent Intelligente** : 6 types d'intentions dÃ©tectÃ©es
3. **Retrieval Hybride** : Vector Search + BM25 + Cross-Encoder Reranking
4. **Recherches SpÃ©cialisÃ©es** : Par rÃ©fÃ©rence (Article X), par juridiction (CCJA), par type de document
5. **MÃ©tadonnÃ©es Enrichies** : Category, Section, File Path pour chaque source
6. **Mode Streaming** : RÃ©ponses progressives via Server-Sent Events (SSE)
7. **Persistance Conversations** : Historique sauvegardÃ© en base PostgreSQL

---

## ğŸ—ï¸ Architecture GÃ©nÃ©rale

### Stack Technologique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAURI CHATBOT SERVICE                    â”‚
â”‚                     (FastAPI - Python 3.11)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚             â”‚             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚   ChromaDB   â”‚ â”‚ DeepSeekâ”‚ â”‚ PostgreSQL â”‚
        â”‚  (Vectors)   â”‚ â”‚  (LLM)  â”‚ â”‚(Convs/Msgs)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composants Externes :**
- **ChromaDB** (port 8000) : Base vectorielle pour embeddings + mÃ©tadonnÃ©es
- **DeepSeek API** : LLM principal (deepseek-chat) via OpenRouter
- **PostgreSQL** (via kauri_user_service) : Stockage conversations et messages
- **User Service** (port 3201) : Authentification JWT

**ModÃ¨les ML :**
- **Embeddings** : `BAAI/bge-m3` (multilingual, 1024 dimensions)
- **Reranking** : `cross-encoder/ms-marco-MiniLM-L-6-v2`
- **LLM Principal** : `deepseek/deepseek-chat` (via OpenRouter)
- **LLM Fallback** : `openai/gpt-4o-mini`

---

## ğŸ”„ Pipeline RAG Conversationnel

### Architecture en 3 Couches

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. API LAYER (FastAPI)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  /chat/query    â”‚              â”‚  /chat/stream   â”‚       â”‚
â”‚  â”‚  (non-stream)   â”‚              â”‚  (SSE stream)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            2. CONVERSATION LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚        ConversationAwareRAG                      â”‚       â”‚
â”‚  â”‚  - RÃ©cupÃ¨re historique conversation             â”‚       â”‚
â”‚  â”‚  - Augmente query avec contexte                 â”‚       â”‚
â”‚  â”‚  - Sauvegarde messages (user + assistant)       â”‚       â”‚
â”‚  â”‚  - Auto-gÃ©nÃ¨re titre conversation               â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            3. RAG PIPELINE LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              RAGWorkflow (LangGraph)             â”‚       â”‚
â”‚  â”‚                                                  â”‚       â”‚
â”‚  â”‚  Node 1: classify_intent                        â”‚       â”‚
â”‚  â”‚         â†“                                        â”‚       â”‚
â”‚  â”‚  Node 2: routing (6 types)                      â”‚       â”‚
â”‚  â”‚         â†“                                        â”‚       â”‚
â”‚  â”‚  Nodes 3-8: SpÃ©cialisÃ©s par intent              â”‚       â”‚
â”‚  â”‚    - general_conversation                       â”‚       â”‚
â”‚  â”‚    - rag_query                                  â”‚       â”‚
â”‚  â”‚    - clarification                              â”‚       â”‚
â”‚  â”‚    - document_sourcing                          â”‚       â”‚
â”‚  â”‚    - legal_reference_search                     â”‚       â”‚
â”‚  â”‚    - case_law_research                          â”‚       â”‚
â”‚  â”‚         â†“                                        â”‚       â”‚
â”‚  â”‚  Node Final: Format response                    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fichiers ClÃ©s

| Fichier | RÃ´le |
|---------|------|
| `src/api/routes/chat.py` | API endpoints (query, stream, health) |
| `src/rag/pipeline/conversation_aware_rag.py` | Couche conversationnelle + persistence |
| `src/rag/pipeline/rag_pipeline.py` | Pipeline RAG de base (retrieval + generation) |
| `src/rag/agents/rag_workflow.py` | Workflow LangGraph (classification + routing) |

### Gestion robuste des sources (2025-11-10)

- Chaque message assistant sauvegarde dÃ©sormais les sources **enrichies** (titre, score, catÃ©gorie, `file_path`,
  rÃ©sumÃ© de mÃ©tadonnÃ©es). Ces objets JSON restent disponibles pour les conversations suivantes.
- Le `ContextManager` maintient un buffer FIFO de rÃ©fÃ©rences dÃ©dupliquÃ©es (`get_recent_sources`) par conversation :
  on peut rÃ©utiliser automatiquement jusquâ€™Ã  5 sources rÃ©centes sans solliciter Ã  nouveau Chroma si elles couvrent dÃ©jÃ 
  la question de suivi.
- Dans le workflow RAG, un retrieval hybride est toujours exÃ©cutÃ©. Lorsque moins de `RAG_MIN_DOCUMENTS` (3) documents
  pertinents sont trouvÃ©s, les sources sont complÃ©tÃ©es avec celles du buffer afin de garantir quâ€™au moins trois
  rÃ©fÃ©rences soient renvoyÃ©es au frontend.
- Le buffer limite aussi la pollution du contexte : seules les rÃ©fÃ©rences utiles sont conservÃ©es et envoyÃ©es au LLM,
  ce qui Ã©vite de saturer rapidement les 8â€¯000 tokens rÃ©servÃ©s au contexte conversationnel.

---

## ğŸ§  Workflow LangGraph

### Architecture du Workflow

Le workflow utilise **LangGraph** pour orchestrer le traitement des requÃªtes via un graphe de nÅ“uds spÃ©cialisÃ©s.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  START (Query)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ classify_intent  â”‚
                    â”‚  (IntentNode)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     ROUTING      â”‚
                    â”‚  (6 branches)    â”‚
                    â””â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”€â”€â”€â”€â”˜
                      â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚ â”‚ â”‚ â”‚                â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â–¼â”€â–¼â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚general_ â”‚  â”‚     rag_query      â”‚   â”‚case_law_   â”‚
   â”‚convers. â”‚  â”‚   (retrieval +     â”‚   â”‚research    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    generation)     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚  RESPONSEâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Types d'Intentions (6)

| Intent | Description | Workflow Node | Retrieval |
|--------|-------------|---------------|-----------|
| **general_conversation** | Questions gÃ©nÃ©rales hors OHADA | `_general_conversation_node` | âŒ Non |
| **rag_query** | Questions nÃ©cessitant documentation OHADA | `_rag_query_node` | âœ… Oui (standard) |
| **clarification** | Demandes de clarification/reformulation | `_clarification_node` | âŒ Non |
| **document_sourcing** | Recherche de documents sources | `_document_sourcing_node` | âœ… Oui (enriched metadata) |
| **legal_reference_search** | Recherche par rÃ©fÃ©rence (Article X, Compte Y) | `_legal_reference_search_node` | âœ… Oui (by reference) |
| **case_law_research** | Recherche jurisprudentielle (CCJA, etc.) | `_case_law_research_node` | âœ… Oui (by jurisdiction) |

### Classification d'Intent

**Fichier** : `src/rag/agents/intent_classifier.py`

**Processus** :
1. Query utilisateur â†’ LLM DeepSeek (tempÃ©rature=0.0 pour dÃ©terminisme)
2. Prompt spÃ©cialisÃ© OHADA avec exemples
3. Parsing JSON de la rÃ©ponse
4. Enrichissement avec `LegalReferenceParser` si rÃ©fÃ©rences dÃ©tectÃ©es

**Exemple de Prompt** :
```
Tu es un classificateur d'intentions pour un assistant juridique OHADA.
Analyse la question et dÃ©termine l'intention parmi : [6 types]

Question : "Que dit l'Article 15 de l'AU-OHADA ?"
â†’ Intent: legal_reference_search
â†’ Legal Metadata: { reference: "Article 15", source: "AU-OHADA" }
```

**Output** :
```python
IntentClassification(
    intent_type="legal_reference_search",
    confidence=0.95,
    reasoning="Question ciblÃ©e sur un article prÃ©cis",
    direct_answer=None,
    legal_metadata=LegalMetadata(
        document_type="acte_uniforme",
        legal_references=[LegalReference(type="article", number="15")]
    )
)
```

---

## ğŸ” Retrieval Hybride

### Architecture 3-Stages

Le systÃ¨me combine **3 mÃ©thodes de retrieval** pour maximiser la pertinence :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STAGE 1: PARALLEL RETRIEVAL                 â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Vector Search   â”‚          â”‚   BM25 Search   â”‚      â”‚
â”‚  â”‚ (Semantic)      â”‚          â”‚   (Keyword)     â”‚      â”‚
â”‚  â”‚                 â”‚          â”‚                 â”‚      â”‚
â”‚  â”‚ Query Embedding â”‚          â”‚ Token Matching  â”‚      â”‚
â”‚  â”‚      â†“          â”‚          â”‚      â†“          â”‚      â”‚
â”‚  â”‚ ChromaDB        â”‚          â”‚ BM25 Index      â”‚      â”‚
â”‚  â”‚ (top_k=20)      â”‚          â”‚ (top_k=20)      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STAGE 2: FUSION                             â”‚
â”‚                                                          â”‚
â”‚  Reciprocal Rank Fusion (RRF)                           â”‚
â”‚  - Combine scores from both retrievers                  â”‚
â”‚  - Deduplicate documents                                â”‚
â”‚  - Rerank by fused score                                â”‚
â”‚  - Output: top_k candidates (default: 10)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STAGE 3: RERANKING                          â”‚
â”‚                                                          â”‚
â”‚  Cross-Encoder Reranking                                â”‚
â”‚  Model: ms-marco-MiniLM-L-6-v2                          â”‚
â”‚  - Rescore each (query, document) pair                  â”‚
â”‚  - Deep semantic understanding                          â”‚
â”‚  - Output: top_k final (default: 5)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fichier Principal

**`src/rag/retriever/hybrid_retriever.py`**

**MÃ©thodes** :
- `retrieve()` : Retrieval hybride standard (Vector + BM25 + Reranking)
- `retrieve_by_metadata()` : Retrieval avec filtrage par mÃ©tadonnÃ©es (category, jurisdiction)

### ParamÃ¨tres de Configuration

```python
# Vector Search
rag_top_k = 20  # Nombre de docs rÃ©cupÃ©rÃ©s par vector search

# Reranking
rag_rerank_top_k = 10  # Candidats avant reranking
rag_final_top_k = 5    # Documents finaux aprÃ¨s reranking

# BM25
bm25_k1 = 1.5  # Saturation term frequency
bm25_b = 0.75  # Document length normalization
```

### Retrieval SpÃ©cialisÃ© (Legal)

**Fichier** : `src/rag/retriever/legal_retriever.py`

**MÃ©thodes SpÃ©cialisÃ©es** :

1. **`retrieve_by_reference(reference: LegalReference)`**
   - Recherche ciblÃ©e par Article, Compte, Chapitre
   - Utilise mÃ©tadonnÃ©es enrichies (`article`, `compte`, `section`)
   - Combine filtrage metadata + vector search

2. **`retrieve_case_law(topic: str, jurisdiction: str)`**
   - Recherche jurisprudentielle par juridiction (CCJA, Cour SuprÃªme, etc.)
   - Filtre `category="jurisprudence"` + `jurisdiction="CCJA"`

3. **`retrieve_by_document_type(type: str)`**
   - Filtre par type : acte_uniforme, plan_comptable, jurisprudence, doctrine

4. **`retrieve_related(reference_doc: Dict)`**
   - Trouve documents similaires/liÃ©s Ã  un document de rÃ©fÃ©rence

---

## ğŸ“Š MÃ©tadonnÃ©es Enrichies

### Extraction Automatique Ã  l'Ingestion

**Fichier** : `src/ingestion/metadata_extractor.py`

Lors de l'ingestion de documents, le systÃ¨me extrait automatiquement :

```python
{
    # Type de document (dÃ©tectÃ© du path + contenu)
    "category": "plan_comptable" | "acte_uniforme" | "jurisprudence" | "doctrine",

    # Structure documentaire
    "section": "partie_1",  # Extrait du path
    "title": "Chapitre 5 OpÃ©rations d'investissement",
    "file_path": "/app/base_connaissances/plan_comptable/partie_1/...",

    # RÃ©fÃ©rences juridiques/comptables
    "articles_references": ["Article 15", "Article 42"],
    "article": "15",  # Article principal
    "comptes_references": ["Compte 6012", "Compte 601"],
    "compte": "6012",
    "classes": ["Classe 6"],
    "classe": "6",

    # MÃ©tadonnÃ©es jurisprudentielles
    "jurisdiction": "CCJA",
    "case_number": "056/2023",
    "date": "2023-05-15",

    # ThÃ©matiques juridiques
    "legal_topics": ["amortissement", "immobilisation"],

    # Statistiques
    "content_length": 15420,
    "word_count": 2500
}
```

### Patterns Reconnus

```python
# Articles
Article 15
Art. 42
Art 35 de l'AU-OHADA

# Comptes
Compte 6012
Compte 601
Classe 6

# Jurisprudences
CCJA/2023/056
ArrÃªt nÂ°056/2023
CCJA, ArrÃªt nÂ°056/2023 du 15 mai 2023

# Juridictions
CCJA (Cour Commune de Justice et d'Arbitrage)
Cour SuprÃªme
Cour d'Appel
Tribunal de Commerce

# Dates
15 mai 2023
8 mars 2022
2023-05-15
```

### Sources Enrichies dans RÃ©ponse API

**Avant (Phase 1)** :
```json
{
  "title": "Plan Comptable / Partie 1 / chapitre_5...",
  "score": 3.04,
  "category": null,
  "section": null,
  "file_path": null,
  "document_type": null
}
```

**AprÃ¨s (Phase 2 - Actuel)** :
```json
{
  "title": "Plan Comptable / Partie 1 / chapitre_5 OpÃ©rations d'investissement",
  "score": 3.039626359939575,
  "category": "plan_comptable",
  "section": "partie_1",
  "file_path": "/app/base_connaissances/plan_comptable/partie_1/chapitre_5.docx",
  "document_type": "plan_comptable",
  "metadata_summary": null
}
```

**Note** : `metadata_summary` est un objet contenant des mÃ©tadonnÃ©es additionnelles spÃ©cifiques (articles, comptes) quand disponibles.

---

## ğŸŒ Endpoints API

### Base URL
```
http://localhost:3202/api/v1
```

### 1. POST `/chat/query` - Query Non-Stream

**Description** : RequÃªte RAG standard avec rÃ©ponse complÃ¨te d'un seul coup.

**Headers** :
```
Authorization: Bearer <JWT_TOKEN>
Content-Type: application/json
```

**Request Body** :
```json
{
  "query": "Qu'est-ce qu'un acte uniforme OHADA ?",
  "conversation_id": "uuid-optional"
}
```

**Response** :
```json
{
  "conversation_id": "uuid",
  "message_id": "uuid",
  "query": "Qu'est-ce qu'un acte uniforme OHADA ?",
  "answer": "Un Acte uniforme OHADA est...",
  "sources": [
    {
      "title": "Acte Uniforme / TraitÃ© / Article 1",
      "score": 0.92,
      "category": "acte_uniforme",
      "section": "traite",
      "file_path": "/app/.../acte_uniforme_traite.pdf",
      "document_type": "acte_uniforme",
      "metadata_summary": {
        "article": "1"
      }
    }
  ],
  "model_used": "deepseek/deepseek-chat",
  "tokens_used": 1234,
  "latency_ms": 2450,
  "metadata": {
    "intent_type": "rag_query",
    "intent_confidence": 0.98,
    "num_sources": 4,
    "retrieval_performed": true,
    "use_reranking": true
  },
  "timestamp": "2025-11-08T06:30:00Z"
}
```

**Codes de Statut** :
- `200 OK` : SuccÃ¨s
- `401 Unauthorized` : JWT invalide/expirÃ©
- `500 Internal Server Error` : Erreur serveur

---

### 2. POST `/chat/stream` - Query Stream (SSE)

**Description** : RequÃªte RAG avec streaming progressif via Server-Sent Events.

**Headers** :
```
Authorization: Bearer <JWT_TOKEN>
Content-Type: application/json
```

**Request Body** :
```json
{
  "query": "Comment comptabiliser un amortissement ?",
  "conversation_id": "uuid-optional"
}
```

**Response** : Stream SSE (`text/event-stream`)

**Ã‰vÃ©nements** :

1. **`sources`** - Sources rÃ©cupÃ©rÃ©es (envoyÃ© en premier)
```
data: {"type":"sources","sources":[...],"metadata":{"num_sources":4}}
```

2. **`token`** - Tokens de rÃ©ponse (streamÃ©s progressivement)
```
data: {"type":"token","content":"Un "}
data: {"type":"token","content":"amortissement "}
data: {"type":"token","content":"est..."}
```

3. **`done`** - MÃ©tadonnÃ©es finales
```
data: {"type":"done","metadata":{"conversation_id":"uuid","model_used":"deepseek/deepseek-chat","tokens_used":345,"latency_ms":1890,"intent_type":"rag_query"}}
```

4. **`message_id`** - ID du message assistant (pour feedback)
```
data: {"type":"message_id","message_id":"uuid"}
```

5. **`error`** - En cas d'erreur
```
data: {"type":"error","content":"Erreur: ..."}
```

---

### 3. GET `/chat/health` - Health Check

**Description** : VÃ©rification de l'Ã©tat du service.

**Response** :
```json
{
  "status": "ok",
  "service": "chat",
  "endpoints": {
    "query": "/api/v1/chat/query",
    "stream": "/api/v1/chat/stream"
  }
}
```

---

## âš™ï¸ Configuration

### Fichier de Configuration

**`src/config.py`** - Configuration via Pydantic Settings

### Variables d'Environnement Principales

#### LLM Configuration
```bash
LLM_PROVIDER=deepseek                    # Provider principal
LLM_MODEL=deepseek-chat                  # ModÃ¨le principal
LLM_TEMPERATURE=0.1                      # TempÃ©rature gÃ©nÃ©ration (0-1, 0.1=dÃ©terministe)
LLM_MAX_TOKENS=2500                      # Max tokens par rÃ©ponse

LLM_FALLBACK_PROVIDER=openai             # Provider de secours
LLM_FALLBACK_MODEL=gpt-4o-mini          # ModÃ¨le de secours

INTENT_CLASSIFIER_TEMPERATURE=0.0        # TempÃ©rature classification (0=ultra-dÃ©terministe)
INTENT_CLASSIFIER_MAX_TOKENS=500         # Max tokens classification
```

#### Embeddings & Retrieval
```bash
EMBEDDER_MODEL=BAAI/bge-m3              # ModÃ¨le embeddings
EMBEDDER_DEVICE=cpu                      # cpu ou cuda
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

RAG_TOP_K=20                             # Docs rÃ©cupÃ©rÃ©s (vector + bm25)
RAG_RERANK_TOP_K=10                      # Candidats avant reranking
RAG_FINAL_TOP_K=5                        # Docs finaux aprÃ¨s reranking
```

#### ChromaDB
```bash
CHROMA_HOST=chromadb
CHROMA_PORT=8000
CHROMA_COLLECTION=kauri_ohada_knowledge
```

#### PostgreSQL (via User Service)
```bash
DATABASE_URL=postgresql://user:pass@host:5432/kauri
```

#### Workflow & Features
```bash
USE_RAG_WORKFLOW=true                    # Activer workflow LangGraph
ENABLE_LEGAL_REPORTS=false               # Rapports structurÃ©s (dÃ©sactivÃ© par dÃ©faut)
REPORT_AUTO_GENERATE_THRESHOLD=3         # Seuil auto-gÃ©nÃ©ration rapport
```

#### API Keys
```bash
OPENROUTER_API_KEY=sk-or-...            # OpenRouter (DeepSeek)
OPENAI_API_KEY=sk-...                    # OpenAI (fallback)
```

---

## ğŸ’¾ Base de DonnÃ©es

### PostgreSQL (via User Service)

**Tables UtilisÃ©es** :

#### `conversations`
```sql
CREATE TABLE conversations (
    id UUID PRIMARY KEY,
    user_id UUID NOT NULL REFERENCES users(id),
    title VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### `messages`
```sql
CREATE TABLE messages (
    id UUID PRIMARY KEY,
    conversation_id UUID NOT NULL REFERENCES conversations(id),
    role VARCHAR(50) NOT NULL,  -- 'user' | 'assistant'
    content TEXT NOT NULL,
    sources JSONB,              -- Array of sources for assistant messages
    metadata JSONB,             -- model_used, tokens_used, intent_type, etc.
    created_at TIMESTAMP DEFAULT NOW()
);
```

**Service** : `src/services/conversation_service.py`

**MÃ©thodes Principales** :
- `get_or_create_conversation()` : RÃ©cupÃ¨re ou crÃ©e conversation
- `save_message()` : Sauvegarde message (user ou assistant)
- `get_conversation_messages()` : RÃ©cupÃ¨re historique (limit=10 derniers)
- `auto_generate_title()` : GÃ©nÃ¨re titre conversation automatiquement

---

### ChromaDB (Vector Database)

**Collection** : `kauri_ohada_knowledge`

**Documents StockÃ©s** : 10,190+ chunks

**Structure d'un Document** :
```python
{
    "id": "uuid",
    "embedding": [1024 floats],  # BAAI/bge-m3
    "document": "contenu textuel du chunk",
    "metadata": {
        # MÃ©tadonnÃ©es de base
        "source": "file_path",
        "category": "plan_comptable",
        "section": "partie_1",
        "title": "Chapitre 5...",
        "file_path": "/app/.../file.docx",

        # MÃ©tadonnÃ©es enrichies (Phase 2)
        "article": "15",
        "articles_references": ["Article 15", "Article 18"],
        "compte": "6012",
        "comptes_references": ["Compte 6012"],
        "jurisdiction": "CCJA",
        "case_number": "056/2023",
        "date": "2023-05-15",
        "legal_topics": ["amortissement"],

        # Stats
        "content_length": 1500,
        "word_count": 250
    }
}
```

---

## ğŸ“‚ Flux de DonnÃ©es

### Flux Complet - Query Non-Stream

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. CLIENT REQUEST                        â”‚
â”‚  POST /api/v1/chat/query                                    â”‚
â”‚  Headers: { Authorization: Bearer JWT }                     â”‚
â”‚  Body: { query, conversation_id }                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2. AUTHENTICATION (JWT)                        â”‚
â”‚  - Validate JWT with User Service                          â”‚
â”‚  - Extract user_id from token                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         3. CONVERSATION LAYER (ConversationAwareRAG)        â”‚
â”‚  a) Get/Create conversation (PostgreSQL)                   â”‚
â”‚  b) Retrieve last 10 messages (PostgreSQL)                 â”‚
â”‚  c) Save user message (PostgreSQL)                         â”‚
â”‚  d) Build conversation context (format history)            â”‚
â”‚  e) Augment query with context                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            4. RAG WORKFLOW (LangGraph)                      â”‚
â”‚  a) classify_intent (LLM DeepSeek, temp=0.0)               â”‚
â”‚     â†’ Output: intent_type + legal_metadata                 â”‚
â”‚  b) routing (based on intent_type)                         â”‚
â”‚     â†’ Route to specialized node                            â”‚
â”‚  c) Execute node (example: rag_query)                      â”‚
â”‚     â†’ Hybrid Retrieval (see next section)                  â”‚
â”‚     â†’ LLM Generation                                        â”‚
â”‚  d) Format response                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          5. HYBRID RETRIEVAL (if needed)                    â”‚
â”‚  a) Parallel Retrieval:                                     â”‚
â”‚     - Vector Search (ChromaDB, top_k=20)                   â”‚
â”‚     - BM25 Search (in-memory index, top_k=20)              â”‚
â”‚  b) Fusion (RRF, deduplicate)                              â”‚
â”‚  c) Reranking (Cross-Encoder, top_k=5)                     â”‚
â”‚  â†’ Output: 5 most relevant documents + metadata            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              6. LLM GENERATION                              â”‚
â”‚  a) Format context from documents                          â”‚
â”‚  b) Build system prompt (OHADA specialist)                 â”‚
â”‚  c) Build user prompt (context + query + rules)            â”‚
â”‚  d) Call LLM DeepSeek (temp=0.1, max_tokens=2500)          â”‚
â”‚  â†’ Output: Generated answer                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          7. SAVE ASSISTANT MESSAGE                          â”‚
â”‚  - Save to PostgreSQL messages table                       â”‚
â”‚  - Include: content, sources, metadata                     â”‚
â”‚  - Auto-generate conversation title (if first message)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              8. FORMAT & RETURN RESPONSE                    â”‚
â”‚  {                                                          â”‚
â”‚    conversation_id, message_id, query, answer,             â”‚
â”‚    sources (enriched), model_used, tokens_used,            â”‚
â”‚    latency_ms, metadata, timestamp                         â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux Complet - Query Stream

**Identique jusqu'Ã  l'Ã©tape 6**, puis :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              6. LLM STREAMING GENERATION                    â”‚
â”‚  a) Yield sources event                                     â”‚
â”‚     data: {"type":"sources","sources":[...]}               â”‚
â”‚  b) Stream tokens progressively                            â”‚
â”‚     data: {"type":"token","content":"Un "}                 â”‚
â”‚     data: {"type":"token","content":"amortissement "}      â”‚
â”‚     ...                                                     â”‚
â”‚  c) Yield message_id event                                 â”‚
â”‚     data: {"type":"message_id","message_id":"uuid"}        â”‚
â”‚  d) Yield done event with metadata                         â”‚
â”‚     data: {"type":"done","metadata":{...}}                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Fichiers ClÃ©s

### Structure du Projet

```
backend/kauri_chatbot_service/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ chat.py                    # Endpoints API (/query, /stream, /health)
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation_aware_rag.py  # Couche conversationnelle + persistence
â”‚   â”‚   â”‚   â””â”€â”€ rag_pipeline.py            # Pipeline RAG de base
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_workflow.py            # LangGraph workflow (6 nodes)
â”‚   â”‚   â”‚   â”œâ”€â”€ intent_classifier.py       # Classification d'intent
â”‚   â”‚   â”‚   â”œâ”€â”€ reference_parser.py        # Parsing rÃ©fÃ©rences juridiques
â”‚   â”‚   â”‚   â””â”€â”€ legal_report_generator.py  # GÃ©nÃ©ration rapports structurÃ©s
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ retriever/
â”‚   â”‚       â”œâ”€â”€ hybrid_retriever.py        # Retrieval hybride (Vector+BM25+Rerank)
â”‚   â”‚       â”œâ”€â”€ legal_retriever.py         # Retrieval spÃ©cialisÃ© juridique
â”‚   â”‚       â”œâ”€â”€ vector_retriever.py        # Vector search (ChromaDB)
â”‚   â”‚       â”œâ”€â”€ bm25_retriever.py          # BM25 keyword search
â”‚   â”‚       â””â”€â”€ reranker.py                # Cross-encoder reranking
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ llm_client.py                  # Client LLM (DeepSeek via OpenRouter)
â”‚   â”‚
â”‚   â”œâ”€â”€ embedder/
â”‚   â”‚   â””â”€â”€ embedder.py                    # Embeddings (BAAI/bge-m3)
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ document_processor.py          # Ingestion documents (PDF, DOCX)
â”‚   â”‚   â””â”€â”€ metadata_extractor.py          # Extraction mÃ©tadonnÃ©es juridiques
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ conversation_service.py        # Persistence conversations (PostgreSQL)
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ chat.py                        # SchÃ©mas Pydantic (Request/Response)
â”‚   â”‚
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â””â”€â”€ jwt_validator.py               # Validation JWT (User Service)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ database.py                    # ModÃ¨les SQLAlchemy
â”‚   â”‚
â”‚   â””â”€â”€ config.py                          # Configuration (Pydantic Settings)
â”‚
â”œâ”€â”€ main.py                                # Application FastAPI
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

### Fichiers de Documentation

```
DOCUMENTATION_TECHNIQUE_COMPLETE.md       # Ce fichier
PHASE1_DOCUMENTATION.md                   # Phase 1: Classification enrichie
PHASE2_DOCUMENTATION.md                   # Phase 2: Rapports + MÃ©tadonnÃ©es
RESUME_AMELIORATIONS.md                   # RÃ©sumÃ© des amÃ©liorations
GUIDE_UTILISATEUR_JURISTE.md              # Guide utilisateur final
ANALYSE_STREAM_VS_NONSTREAM.md            # Analyse tests stream/non-stream
```

---

## ğŸ” SÃ©curitÃ© & Authentification

### JWT Validation

**Fichier** : `src/auth/jwt_validator.py`

**Processus** :
1. Extraction du token depuis header `Authorization: Bearer <token>`
2. Validation du token avec User Service (HTTP request)
3. VÃ©rification expiration + signature
4. Extraction user_id, email, subscription info
5. Injection dans request state

**Protection Endpoints** :
- âœ… `/chat/query` : ProtÃ©gÃ© (JWT requis)
- âœ… `/chat/stream` : ProtÃ©gÃ© (JWT requis)
- âŒ `/chat/health` : Public (pas de JWT)

---

## ğŸ“Š MÃ©triques & Performance

### Latences Typiques

| Endpoint | Mode | Latence Moyenne | DÃ©tails |
|----------|------|-----------------|---------|
| `/chat/query` | Non-stream | ~20-25s | Retrieval (2-3s) + Generation (15-20s) |
| `/chat/stream` | Stream | ~18-20s | MÃªme mais perception plus rapide |

**Breakdown Latence** :
- Classification intent : ~1-2s
- Retrieval (vector + BM25) : ~1s
- Reranking : ~0.5-1s
- LLM Generation : ~15-20s (dÃ©pend longueur rÃ©ponse)
- Persistence DB : ~0.5s

### Optimisations AppliquÃ©es

1. **Lazy Loading** : Embeddings et reranker chargÃ©s Ã  la premiÃ¨re utilisation
2. **Caching** : BM25 index reconstruit au dÃ©marrage puis gardÃ© en mÃ©moire
3. **Parallel Retrieval** : Vector et BM25 en parallÃ¨le
4. **Temperature Optimization** : 0.1 pour gÃ©nÃ©ration, 0.0 pour classification (dÃ©terminisme)
5. **Streaming** : Permet UI responsive pendant gÃ©nÃ©ration

---

## ğŸš€ DÃ©ploiement

### Docker Compose

**Service** : `kauri_chatbot_service`

```yaml
kauri_chatbot_service:
  build: ./backend/kauri_chatbot_service
  container_name: kauri_chatbot_service
  ports:
    - "3202:3202"
  environment:
    - DATABASE_URL=${DATABASE_URL}
    - CHROMA_HOST=chromadb
    - CHROMA_PORT=8000
    - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
  depends_on:
    - chromadb
    - kauri_user_service
  volumes:
    - ./base_connaissances:/app/base_connaissances:ro
```

### Commandes Utiles

```bash
# RedÃ©marrer le service
docker-compose restart kauri_chatbot_service

# Voir les logs
docker logs kauri_chatbot_service --tail 100 -f

# Health check
curl http://localhost:3202/api/v1/chat/health
```

---

## ğŸ§ª Tests

### Scripts de Test

```
test_stream_vs_nonstream.py          # Compare stream vs non-stream
test_sources_metadata.py              # VÃ©rifie mÃ©tadonnÃ©es enrichies
test_phase1_enhancements.py           # Tests Phase 1 (classification)
```

### Test Manuel

```bash
# Obtenir token JWT
TOKEN=$(curl -s --request POST \
  --url http://localhost:3201/api/v1/auth/login \
  --header 'content-type: application/json' \
  --data '{"email":"user@example.com","password":"pass"}' \
  | jq -r '.access_token')

# Test query non-stream
curl --request POST \
  --url http://localhost:3202/api/v1/chat/query \
  --header "Authorization: Bearer $TOKEN" \
  --header 'content-type: application/json' \
  --data '{"query":"Qu'\''est-ce qu'\''un acte uniforme OHADA ?"}'
```

---

## ğŸ“ˆ AmÃ©liorations Futures

### Phase 3 (Optionnel)

1. **Export PDF/DOCX** : Rapports juridiques exportables
2. **Templates Personnalisables** : Cabinets peuvent dÃ©finir formats
3. **Analyse Comparative Automatique** : Comparer jurisprudences/doctrines
4. **Cache Intelligent** : Rapports frÃ©quents prÃ©-gÃ©nÃ©rÃ©s
5. **Fine-tuning Reranker** : SpÃ©cialisÃ© sur corpus OHADA
6. **API DÃ©diÃ©e Rapports** : Endpoint `/api/v1/reports/generate`
7. **Multi-langue** : Support Anglais pour OHADA anglophone

---

## ğŸ“ Support & Contact

**Projet** : KAURI - Assistant Juridique et Comptable OHADA
**Version** : 2.1.0
**DerniÃ¨re Mise Ã  Jour** : 08/11/2025
**DÃ©veloppÃ© avec** : Claude Code (Anthropic)

---

**Fin de la Documentation Technique ComplÃ¨te**
